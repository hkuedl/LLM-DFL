{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a41b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093ead2bbb684dfea033a50307e003af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading checkpoint shards', max=4.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda:1'\n",
    "# 加载模型和分词器\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbac47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_generation(prompt: str) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # 生成回复（设置生成参数）\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,       # 最大生成token数\n",
    "        temperature=0.7,          # 控制随机性（0-1，越高越随机）\n",
    "        top_p=0.9,                # 核采样参数\n",
    "        do_sample=True,           # 启用采样\n",
    "        pad_token_id=tokenizer.eos_token_id  # 避免警告\n",
    "    )\n",
    "    \n",
    "    # 解码并返回结果\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22249697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question='In power systems, we will forecasts the load in the next and dispatch generators, is the impact of overestimation and underestimation of load\\\n",
    "# the same on downstream decision-making?'\n",
    "\n",
    "# response = basic_generation(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to the document for workspace information: https://help.aliyun.com/document_detail/2746874.html    \n",
    "        \n",
    "from http import HTTPStatus\n",
    "import dashscope\n",
    " \n",
    "def call_with_stream():\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': '介绍一下你自己'}]\n",
    "    responses = dashscope.Generation.call(\"qwen-max\", #调用的模型接口\n",
    "                                messages=messages,\n",
    "                                result_format='message',  # set the result to be \"message\"  format.\n",
    "                                stream=True, # 是否开启流式输出，不开启设置False\n",
    "                                incremental_output=True  # 是否开启流式输出，不开启设置False\n",
    "                                )\n",
    "    for response in responses:\n",
    "        if response.status_code == HTTPStatus.OK:\n",
    "            print(response.output.choices[0]['message']['content'],end='')\n",
    "        else:\n",
    "            print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "                response.request_id, response.status_code,\n",
    "                response.code, response.message\n",
    "            ))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "        call_with_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84a9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条: id=1, prompt=a, distance=0.2449\n",
      "第2条: id=3, prompt=c, distance=0.2449\n",
      "id=1的新数据： {'id': 1, 'flag': 1, 'prompt': 'a', 'load': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], 'fine_tune_load': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], 'cost': 9.9}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CurveDictDB:\n",
    "    def __init__(self):\n",
    "        self.data = {}   # key: id, value: 记录字典\n",
    "    \n",
    "    def insert(self, record):\n",
    "        \"\"\"\n",
    "        插入一条新的实验记录，id 必须唯一\n",
    "        \"\"\"\n",
    "        record_id = record['id']\n",
    "        if record_id in self.data:\n",
    "            raise ValueError(f\"id {record_id} 已存在，不能重复插入！\")\n",
    "        self.data[record_id] = record\n",
    "    \n",
    "    def get_top_k(self, curve_new, k=3, distance_type='euclidean'):\n",
    "        \"\"\"\n",
    "        查找与 curve_new 最近的 k 条记录\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for record_id, record in self.data.items():\n",
    "            pre_curve = record['load']\n",
    "            if distance_type == 'euclidean':\n",
    "                dist = np.linalg.norm(np.array(curve_new) - np.array(pre_curve))\n",
    "            elif distance_type == 'manhattan':\n",
    "                dist = np.sum(np.abs(np.array(curve_new) - np.array(pre_curve)))\n",
    "            else:\n",
    "                raise ValueError(f\"未知的distance_type: {distance_type}\")\n",
    "            results.append({\n",
    "                'id': record_id,\n",
    "                'prompt': record['prompt'],\n",
    "                'distance': dist,\n",
    "                'record': record\n",
    "            })\n",
    "        results.sort(key=lambda x: x['distance'])\n",
    "        return results[:k]\n",
    "\n",
    "    def update(self, record_id, new_data):\n",
    "        \"\"\"\n",
    "        更新指定 id 的实验记录\n",
    "        \"\"\"\n",
    "        if record_id in self.data:\n",
    "            self.data[record_id].update(new_data)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_by_id(self, record_id):\n",
    "        \"\"\"\n",
    "        根据 id 获取实验记录\n",
    "        \"\"\"\n",
    "        return self.data.get(record_id)  # 找不到返回 None\n",
    "\n",
    "    def all(self):\n",
    "        \"\"\"\n",
    "        返回所有记录的列表\n",
    "        \"\"\"\n",
    "        return list(self.data.values())\n",
    "\n",
    "# ------ 使用举例 -------\n",
    "\n",
    "db = CurveDictDB()\n",
    "# 插入三条数据\n",
    "db.insert({'id':1,'flag': 1, 'prompt': 'a', 'load': [0.1]*24,'fine_tune_load': [0.1]*24, 'cost': 3.2})\n",
    "db.insert({'id':2,'flag': 0, 'prompt': 'b', 'load': [0.5]*24, 'fine_tune_load': [0.5]*24, 'cost': 5.1})\n",
    "db.insert({'id':3,'flag': 0, 'prompt': 'c', 'load': [0.2]*24, 'fine_tune_load': [0.2]*24, 'cost': 6.7})\n",
    "\n",
    "curve_new = [0.15]*24\n",
    "\n",
    "topk = db.get_top_k(curve_new, k=2)\n",
    "for i, item in enumerate(topk):\n",
    "    print(f\"第{i+1}条: id={item['id']}, prompt={item['prompt']}, distance={item['distance']:.4f}\")\n",
    "\n",
    "# 更新\n",
    "db.update(1, {'cost': 9.9})\n",
    "\n",
    "# 查询\n",
    "print(\"id=1的新数据：\", db.get_by_id(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d4b48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurvePromptBuilder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def format_curve(curve):\n",
    "        \"\"\"\n",
    "        Formats the curve array as a string.\n",
    "        \"\"\"\n",
    "        return \"[\" + \", \".join(f\"{x:.4f}\" for x in curve) + \"]\"\n",
    "\n",
    "    def format_record(self, record):\n",
    "        \"\"\"\n",
    "        record: a single record dictionary from CurveDictDB\n",
    "        \"\"\"\n",
    "        text = f\"Record ID: {record.get('id', '')}\\n\"\n",
    "        text += f\"This curve has been fine-tuned by LLM: {record.get('flag', '')}\\n\"\n",
    "        text += f\"Prompt used at the time: {record.get('prompt', '')}\\n\"\n",
    "        text += f\"Cost: {record.get('cost', '')}\\n\"\n",
    "        text += f\"Load curve: {self.format_curve(record['load'])}\\n\"\n",
    "        return text\n",
    "\n",
    "    def build_prompt(self, current_curve, topk_results):\n",
    "        \"\"\"\n",
    "        current_curve: The current prediction curve\n",
    "        topk_results: The list returned by CurveDictDB.get_top_k()\n",
    "        \"\"\"\n",
    "        prompt = \"The current prediction curve is: \"\n",
    "        prompt += self.format_curve(current_curve) + \"\\n\"\n",
    "        prompt += \"The Top-K most similar historical records are:\\n\"\n",
    "        for idx, item in enumerate(topk_results):\n",
    "            prompt += f\"---- Record {idx+1} ----\\n\"\n",
    "            prompt += self.format_record(item['record'])\n",
    "            prompt += f\"Euclidean distance to the current curve: {item['distance']:.4f}\\n\"\n",
    "        prompt += \"Based on the historical fine-tuning results, please fine-tune the current load curve to achieve a lower cost.\\n\"\n",
    "        return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be18f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current prediction curve is: [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500]\n",
      "The Top-K most similar historical records are:\n",
      "---- Record 1 ----\n",
      "Record ID: 1\n",
      "This curve has been fine-tuned by LLM: 1\n",
      "Prompt used at the time: a\n",
      "Cost: 9.9\n",
      "Load curve: [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000]\n",
      "Euclidean distance to the current curve: 0.2449\n",
      "---- Record 2 ----\n",
      "Record ID: 3\n",
      "This curve has been fine-tuned by LLM: 0\n",
      "Prompt used at the time: c\n",
      "Cost: 6.7\n",
      "Load curve: [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000]\n",
      "Euclidean distance to the current curve: 0.2449\n",
      "Based on the historical fine-tuning results, please fine-tune the current load curve to achieve a lower cost.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curve_new = [0.15]*24\n",
    "topk_results = db.get_top_k(curve_new, k=2)  # 用 get_top_k 得到检索结果\n",
    "\n",
    "builder = CurvePromptBuilder()\n",
    "prompt = builder.build_prompt(curve_new, topk_results)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
